{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-24T14:01:58.322140Z","iopub.status.busy":"2024-06-24T14:01:58.321411Z","iopub.status.idle":"2024-06-24T14:01:58.327006Z","shell.execute_reply":"2024-06-24T14:01:58.326190Z","shell.execute_reply.started":"2024-06-24T14:01:58.322105Z"},"trusted":true},"outputs":[],"source":["import os\n","\n","print(os.listdir(\"/kaggle/working\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-24T14:01:58.332399Z","iopub.status.busy":"2024-06-24T14:01:58.331747Z","iopub.status.idle":"2024-06-24T14:09:24.001414Z","shell.execute_reply":"2024-06-24T14:09:24.000494Z","shell.execute_reply.started":"2024-06-24T14:01:58.332373Z"},"trusted":true},"outputs":[],"source":["!pip install dlib torchinfo torchmetrics pytorch-nlp\n","!pip uninstall numpy\n","!pip install numpy==1.26.4"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-24T14:09:24.003657Z","iopub.status.busy":"2024-06-24T14:09:24.003348Z","iopub.status.idle":"2024-06-24T14:09:24.009293Z","shell.execute_reply":"2024-06-24T14:09:24.008311Z","shell.execute_reply.started":"2024-06-24T14:09:24.003629Z"},"trusted":true},"outputs":[],"source":["# Pytorch imports\n","import torch\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","from torch import nn\n","import torchmetrics\n","import torchinfo\n","\n","# Utils imports\n","import numpy as np\n","import os\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{},"source":["# GNLDataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-24T14:09:24.011122Z","iopub.status.busy":"2024-06-24T14:09:24.010774Z","iopub.status.idle":"2024-06-24T14:09:27.845000Z","shell.execute_reply":"2024-06-24T14:09:27.844227Z","shell.execute_reply.started":"2024-06-24T14:09:24.011085Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["import os\n","import dlib\n","import cv2\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset\n","from torchnlp.encoders import LabelEncoder\n","\n","debug_dl = True\n","\n","class GNLDataLoader(Dataset):\n","    \"\"\"Creates a dataloader for the Lipsync Project\"\"\"\n","    face_detector = dlib.get_frontal_face_detector()\n","    landmark = dlib.shape_predictor(\"shape_predictor_68_face_landmarks_GTX.dat\")\n","\n","    alphabet = [x for x in \"abcdefghijklmnopqrstuvwxyz0123456789 \"]\n","    encoder = LabelEncoder(alphabet, reserved_labels=['unknown'], unknown_index=0)\n","    CROPMARGIN = 20\n","\n","    def __init__(self, labels_path: str, data_path: str, transform = None, train_test_percent: int = 75, debug: bool = False) -> None:\n","        \"\"\"\n","        Creates a dataset given the path to the labels and the image directory\n","\n","        Parameters:\n","            - `labels_path`: the path to the `csv` file containing the labels;\n","            - `images_dir`: the path to the directory with the images;\n","            - `transform`: states whether a transformation should be applied to the images or not.\n","        \"\"\"\n","        super().__init__()\n","        self.debug: bool = debug\n","\n","        if self.debug:\n","            print(f\"[DEBUG] The data dir has{' ' if os.path.isdir(data_path) else ' not '}been recognized\")\n","            print(f\"[DEBUG] The label dir has{' ' if os.path.isdir(labels_path) else ' not '}been recognized\")\n","\n","        self.data_path, self.labels_path = data_path, labels_path\n","        self.data_dir, self.labels_dir = sorted(os.listdir(data_path)), sorted(os.listdir(labels_path))\n","        self.transform = transform\n","\n","\n","    def __len__(self) -> int:\n","        \"\"\"\n","        Returns the length of the data/labels folder\n","\n","        Returns:\n","            - `length` (`int`): the length of the data/labels folder\n","        \"\"\"\n","        return len(self.data_dir)\n","\n","\n","    def __getitem__(self, index: int, straight: bool = False) -> tuple[torch.Tensor, list[str]]:\n","        \"\"\"\n","        Get the ith item(s) in the dataset\n","\n","        Parameters:\n","            - `index`: the index of the image that must be retrieven.\n","\n","        Returns:\n","            - (`item`, `label`) (`tuple[torch.Tensor, torch.Tensor]`): the item in the ith position in the dataset, along with its label.\n","        \"\"\"\n","\n","        if self.debug:\n","            print(f\"[DEBUG] Index of the dataloader: {index}\")\n","            print(f\"[DEBUG] Data folder: {self.data_dir[index]}\")\n","            print(f\"[DEBUG] Labels folder: {self.labels_dir[index]}\")\n","\n","        datas = [self.data_dir[index]] if type(self.data_dir[index]) != list else self.data_dir[index]\n","        labels = [self.labels_dir[index]] if type(self.labels_dir[index]) != list else self.labels_dir[index]\n","\n","        to_return = []\n","\n","        for ind, item in enumerate(datas):\n","            to_return.append((self.__load_video__(item), self.__load_label__(labels[ind])))\n","\n","\n","        '''return (\n","            [self.__load_video__(data_piece) for data_piece in datas],\n","            [self.__load_label__(label_piece) for label_piece in labels]\n","        )'''\n","\n","        # print(f\"{len(to_return)}\")\n","        return tuple(to_return)\n","\n","\n","    def __load_video__(self, video_path: str) -> torch.Tensor:\n","        \"\"\"\n","        Loads a video from the dataset given its path\n","\n","        Parameters:\n","            - `video_path`: the path of the video that must be loaded\n","\n","        Returns:\n","            - `video` (`torch.Tensor`): the video as a PyTorch's `Tensor`\n","        \"\"\"\n","        label_name = video_path[:-3] + \"json\"\n","        video_path = os.path.join(self.data_path, video_path)\n","        cap = cv2.VideoCapture(video_path)\n","        if self.debug:\n","            #print(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","            print(f\"[DEBUG] Trying to open the video at path {video_path}\")\n","        to_return = np.ndarray(shape =(75,100,150))\n","\n","        # homog, prev_frame = True, None\n","\n","        for i in range(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))):\n","            _, frame = cap.read()\n","            gframe = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY).astype('uint8')  # Format to 8-bit image. 'int8' doesn't seem to do the job either\n","            '''if self.debug:\n","\n","                cv2.imshow(\"Frame\", gframe)\n","                cv2.waitKey(0)\n","                cv2.destroyAllWindows()\n","                cv2.imwrite(\"/workspace/GUNILEO/tests/gframe001.jpg\", gframe)\n","\n","                prev_frame = gframe.shape if prev_frame == None else prev_frame\n","                homog = False if prev_frame != gframe.shape else True\n","                print(gframe.shape, homog)'''\n","\n","            facedetect = self.face_detector(gframe)\n","\n","            #HAVE A CHECK IF THE FACE IS FOUND OR NOT\n","\n","            try:\n","                face_landmarks = self.landmark(gframe, facedetect[0])\n","                xleft = face_landmarks.part(48).x - self.CROPMARGIN\n","                xright = face_landmarks.part(54).x + self.CROPMARGIN\n","                ybottom = face_landmarks.part(57).y + self.CROPMARGIN\n","                ytop = face_landmarks.part(50).y - self.CROPMARGIN\n","\n","                mouth = gframe[ytop:ybottom, xleft:xright]\n","                mouth = cv2.resize(mouth, (150, 100))\n","\n","                mean = np.mean(mouth)\n","                std_dev = np.std(mouth)\n","                mouth = (mouth - mean) / std_dev\n","                to_return[i] = mouth\n","            except IndexError:\n","                # naughty_boys.add(video_path)\n","                # naughty_labels.add(\"data/matching/labels/\" + label_name)\n","                to_return[i] = np.zeros((100, 150))\n","\n","        cap.release()\n","\n","        if self.debug:\n","            print(f\"[DEBUG] Video {video_path} opened\")\n","            print(f\"[DEBUG] Shape of video: {to_return.shape}\")\n","\n","        to_return = np.array([to_return])\n","\n","        return torch.tensor(to_return, dtype=torch.float32)\n","\n","\n","    def __load_label__(self, label_path: str) -> torch.Tensor:\n","        \"\"\"\n","        Loads a label from the dataset given its path\n","\n","        Parameters:\n","            - `label_path`: the path of the label that must be loaded;\n","\n","        Returns:\n","            - `label` (`torch.Tensor`): the label as a PyTorch's tensor\n","        \"\"\"\n","\n","        encoding = [\n","            {\"b\":\"bin\",\"l\":\"lay\",\"p\":\"place\",\"s\":\"set\"},\n","            {\"b\":\"blue\",\"g\":\"green\",\"r\":\"red\",\"w\":\"white\"},\n","            {\"a\":\"at\",\"b\":\"by\",\"i\":\"in\",\"w\":\"with\"},\n","            \"letter\",\n","            {\"z\":\"zero\",\"1\":\"one\",\"2\":\"two\",\"3\":\"three\",\"4\":\"four\",\"5\":\"five\",\"6\":\"six\",\"7\":\"seven\",\"8\":\"eight\",\"9\":\"nine\"},\n","            {\"a\":\"again\",\"n\":\"now\",\"p\":\"please\",\"s\":\"soon\"}\n","            ]\n","\n","        code = label_path.split(\".\")[0].split(\"_\")[-1]\n","\n","        sentence = []\n","        for i, letter in enumerate(code):\n","            corresponding_dict = encoding[i]\n","            next = letter if corresponding_dict == \"letter\" else corresponding_dict[letter]\n","            sentence = sentence + [\" \"] + [x for x in next]\n","\n","        # Adapting the labels to be all of equal size\n","        for i in range(37 - len(sentence)):\n","            sentence.append(\" \")\n","\n","        enl = self.encoder.batch_encode(sentence)\n","        enl = enl.type(torch.FloatTensor)\n","        if self.debug: print(f\"[DEBUG] Label: {enl}\\n[DEBUG] Sentence: {sentence}\\n[DEBUG] Length: {len(sentence)}\\n\")\n","        return enl\n"]},{"cell_type":"markdown","metadata":{},"source":["# CNN"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-24T14:09:27.847201Z","iopub.status.busy":"2024-06-24T14:09:27.846912Z","iopub.status.idle":"2024-06-24T14:09:27.862304Z","shell.execute_reply":"2024-06-24T14:09:27.861450Z","shell.execute_reply.started":"2024-06-24T14:09:27.847178Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["import torch\n","from torch import nn\n","import torchinfo\n","\n","class SelectItem(nn.Module):\n","    def __init__(self, item_index):\n","        super(SelectItem, self).__init__()\n","        self._name = 'selectitem'\n","        self.item_index = item_index\n","\n","    def forward(self, inputs):\n","        return inputs[self.item_index]\n","    \n","class LabialCNN(nn.Module):\n","    def __init__(self, debug: bool = False):\n","        super().__init__()\n","\n","        self.debug = debug\n","        self.cnn = nn.Sequential(\n","            nn.Conv3d(in_channels=1, out_channels=8, kernel_size=(3, 5, 5), padding=(1, 2, 2), stride=(1, 2, 2)),\n","            nn.ReLU(),\n","            nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2)),\n","        \n","            nn.Conv3d(in_channels=8, out_channels=16, kernel_size=(3, 5, 5), padding=(1, 2, 2), stride=(1, 1, 1)),\n","            nn.ReLU(),\n","            nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2)),\n","        \n","            nn.Conv3d(in_channels=16, out_channels=32, kernel_size=(3, 5, 5), padding=(1, 2, 2), stride=(1, 1, 1)),\n","            nn.ReLU(),\n","            nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2)),\n","             # Left as default, check later if it causes problems\n","            \n","        )    \n","        self.gru = nn.Sequential(\n","            nn.GRU(input_size=1728, hidden_size=256, num_layers=2, dropout=0.5, bidirectional=True),\n","            SelectItem(0),\n","\n","            nn.Linear(in_features=512, out_features=38),\n","            nn.Softmax()\n","        )\n","\n","    # Remember to put FALSE\n","    def forward(self, x):\n","        x = self.cnn(x) # Run through the model\n","        \n","        sh = x.shape\n","        x = torch.reshape(x, (sh[1], sh[0], sh[2], sh[3])) # Reshape so that the channels are flattened, not frames\n","        x = nn.Flatten()(x)\n","        x = self.gru(x)\n","      \n","        \n","        if self.debug: print(f\"Layer's shape: {sh}\")\n","        #x = torch.flatten(x, 1)     # Flatten layer\n","        #if debug: print(f\"  Layer's shape: {x.shape}\")\n","        if self.debug: print(f\"Summary of the layer: a\")\n","\n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["# Loops"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-24T14:09:27.863765Z","iopub.status.busy":"2024-06-24T14:09:27.863464Z","iopub.status.idle":"2024-06-24T14:09:27.879999Z","shell.execute_reply":"2024-06-24T14:09:27.879202Z","shell.execute_reply.started":"2024-06-24T14:09:27.863733Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["import numpy as np\n","import torchmetrics\n","import torch\n","from torch import nn\n","\n","metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=38)\n","batch_size = 32\n","\n","def train_loop(device, dataloader, model, loss_fn, optimizer, batch_index: int, epochs: int, epoch: int, debug: bool=True):\n","    \"\"\"Trains an epoch of the model\n","\n","    Parameters:\n","        - `device`: destination device\n","        - `dataloader`: the dataloader of the dataset\n","        - `model`: the model used\n","        - `loss_fn`: the loss function of the model\n","        - `optimizer`: the optimizer\n","        - `batch_index`: the number of the currently processed batch\n","        - `epochs`: the number of epochs\n","        - `epoch`: the index of the epoch\n","        - `debug`: (default `True`): prints debug info\n","    \"\"\"\n","    model.train()\n","    size = len(dataloader)\n","    predictions = torch.zeros((batch_size, 75, 38)).to(device)  #np.ndarray(shape=(batch_size, 75, 38))\n","    labels = torch.zeros((batch_size, 37)).to(device)  #np.ndarray(shape=(batch_size, 37))\n","\n","    # Get the item from the dataset\n","    for item, (x, y) in enumerate(dataloader):\n","        #print(f\"{x} -> {x.shape}\")\n","        #for index, video in enumerate(x):\n","            # Move data to the device used\n","        video = x.to(device)\n","        label = y.to(device)\n","\n","        # Compute the prediction and the loss\n","        pred = model(video)\n","        predictions[item] = pred\n","        labels[item] = label\n","\n","            # if debug: print(video, video.shape, pred, pred.shape, label, label.shape, sep=\"\\n\\n========================================================\\n\\n\")\n","        # total_acc = metric(pred.permute(1, 0), label)\n","        # print(f\"[DEBUG] Accuracy: {total_acc}\")\n","\n","        # if debug: print(f\"[DEBUG] Preds: {pred.shape}\\n[DEBUG] Label: {label.shape}\")\n","\n","    loss = loss_fn(\n","        predictions.permute(1, 0, 2),\n","        labels,\n","        torch.full(size=(batch_size, ), fill_value=75, dtype=torch.long), # torch.Size([32])\n","        torch.full(size=(batch_size, ), fill_value=37, dtype=torch.long)  # torch.Size([32])\n","    )\n","\n","    # Adjust the weights\n","    # mean_loss = total_loss//batch_size\n","    # avg_acc=total_acc//batch_size\n","    optimizer.step()\n","    loss.backward()\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","    optimizer.zero_grad()\n","\n","    if debug: print(f\"→ Loss: {loss} [Batch {batch_index + 1}/{size}, Epoch {epoch + 1}/{epochs}]\")\n","\n","    \"\"\"predictions = torch.stack(predictions)\n","    labels = torch.stack(labels)\n","    preds_shape = predictions.shape\n","    labels_shape = labels.shape\n","    predictions = torch.reshape(predictions, (preds_shape[1], preds_shape[0], preds_shape[2]))\n","    \"\"\"\n","\n","    \"\"\"\n","    print(\n","    f\"Predictions:\\n{predictions}\\n\\nSize of predictions: {preds_shape}\",\n","    f\"Labels:\\n{y}\\n\\nLabels shape: {y.shape}\",\n","    f\"Input size:\\n{torch.full(size=(batch_size, ), fill_value=75, dtype=torch.long)}\",\n","    f\"Labels size:\\n{torch.full(size=(batch_size, ), fill_value=37, dtype=torch.long)}\",\n","    sep=\"\\n\\n===============================================\\n\\n\"\n","    )\n","    \"\"\"\n","\n","\n","    # Print some information\n","\n","    # if debug: print(f\"Accuracy of item {item}/{size}: {GNLAccuracy(predictions, y)}\")\n","\n","    #accuracy = metric.compute()\n","    print(f\"===     The batch {batch_index + 1}/125 has finished training     ===\")\n","    #if debug: print(f\"→ Final accuracy of the epoch: {accuracy}\")\n","    #metric.reset()\n","\n","\n","def GNLAccuracy(preds, labels) -> float:\n","    alphabet = [x for x in \"abcdefghijklmnopqrstuvwxyz0123456789 \"]\n","    total = 0\n","    for index, video in enumerate(preds):\n","        correct = 0\n","        pred_label = []\n","        label = [i for i in labels[index] if i != \" \"]\n","        for frame in video:\n","            letter = alphabet[torch.argmax(frame)]\n","            if letter != \" \": pred_label.append(letter)\n","\n","        for i, c in enumerate(pred_label):\n","            if c == label[i]:\n","                correct += 1\n","        total += correct / len(pred_label)\n","    return total / batch_size\n","\n","\n","def test_loop(device, dataloader, model, loss_fn, debug=True):\n","    model.eval()\n","    size = len(dataloader)\n","\n","    # Disable the updating of the weights\n","    with torch.no_grad():\n","        for item, (x, y) in enumerate(dataloader):\n","            # Move data to the device used\n","            video = x.to(device)\n","            label = y.to(device)\n","\n","            # Compute the prediction and the loss\n","            pred = model(video)\n","\n","            # Get the accuracy score\n","            acc = metric(pred, label)\n","            acc = metric.compute()\n","            if debug: print(f\"→ Accuracy for image {item}: {acc}\")\n","    # if debug: print(f\"→ Final testing accuracy of the model: {acc}\")\n","    # metric.reset()\n"]},{"cell_type":"markdown","metadata":{},"source":["# Data Loading"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-24T14:09:27.881732Z","iopub.status.busy":"2024-06-24T14:09:27.881180Z","iopub.status.idle":"2024-06-24T14:58:32.523717Z","shell.execute_reply":"2024-06-24T14:58:32.522703Z","shell.execute_reply.started":"2024-06-24T14:09:27.881702Z"},"trusted":true},"outputs":[],"source":["# Create the dataloaders of our project\n","path_data = \"/kaggle/input/gunileo/matching/fronts\" # \"data/lombardgrid_front/lombardgrid/front\"\n","path_labels = \"/kaggle/input/gunileo/matching/labels\" # \"data/lombardgrid_alignment/lombardgrid/alignment\"\n","\n","dataset = GNLDataLoader(path_labels, path_data, transform=None, debug=False)\n","\n","print(\n","    f\"[DEBUG] Items in the data folder: {len(sorted(os.listdir(path_data)))}\",\n","    f\"[DEBUG] Items in the labels folder: {len(sorted(os.listdir(path_labels)))}\",\n","    sep=\"\\n\"\n",")\n","\n","# Hyperparameters\n","\n","batch_size = 32"]},{"cell_type":"markdown","metadata":{},"source":["# Model + Hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-24T14:58:32.525258Z","iopub.status.busy":"2024-06-24T14:58:32.524881Z","iopub.status.idle":"2024-06-24T14:58:33.591895Z","shell.execute_reply":"2024-06-24T14:58:33.591016Z","shell.execute_reply.started":"2024-06-24T14:58:32.525230Z"},"trusted":true},"outputs":[],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","model = LabialCNN(debug=False).to(device)\n","\n","# Print the summary of the model\n","# torchinfo.summary(model, (1,75, 100, 150), col_names = (\"input_size\", \"output_size\", \"num_params\", \"kernel_size\", \"mult_adds\"), verbose = 1)\n","\n","epochs = 2\n","folds = 5\n","learning_rate = 10 ** (-4)\n","dropout = 0.5\n","\n","metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=37)\n","\n","loss_fn = nn.CTCLoss(reduction=\"mean\", zero_infinity=True, blank=36)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate"]},{"cell_type":"markdown","metadata":{},"source":["# Training + Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-24T14:58:33.593314Z","iopub.status.busy":"2024-06-24T14:58:33.592986Z","iopub.status.idle":"2024-06-24T14:59:13.225648Z","shell.execute_reply":"2024-06-24T14:59:13.224430Z","shell.execute_reply.started":"2024-06-24T14:58:33.593289Z"},"trusted":true},"outputs":[],"source":["for epoch_ind in range(epochs): # Epochs\n","    index = 0\n","    for fold in range(folds):   # k-fold Cross Validation\n","        for batch_index in range(125 // folds):    # 125\n","            print(f\"[DEBUG] Loading of batch {index + 1} for training (Index: {index})\")\n","            current_batch = dataset[batch_size*index : batch_size*(index + 1)]\n","            #print(f\"[DEBUG] {type(current_batch), len(current_batch)}\\n-> {type(current_batch[0]), len(current_batch[0])}\\n-> {current_batch[0][0].shape}\")\n","\n","            print(f\"[DEBUG] Starting training of batch {batch_index + 1} (Index: {batch_index})\")\n","            train_loop(device, current_batch, model, loss_fn, optimizer, index, epochs, epoch_ind, debug=True)\n","            index += 1\n","        print(\"===          The training has finished          ===\")\n","        for batch_index in range(35 // folds):    # 35\n","            print(f\"[DEBUG] Loading of batch {index + 1} for testing (Index: {index})\")\n","            current_batch = dataset[batch_size*index : batch_size*(index + 1)]\n","\n","            print(f\"[DEBUG] Starting testing of batch {index + 1} (Index: {index})\")\n","            test_loop(device, current_batch, model, loss_fn, debug=True)\n","            index += 1\n","        print(\"===          The testing has finished          ===\")\n","        print(f\"===              Finished fold {fold}/{folds}              ===\")\n","print(\"=== === ==> SAVING THE MODEL...<== === ===\")\n","torch.save(model, \"/kaggle/working/gunileo.pt\")\n","print(\"Goodbye, and thank you for all the fish\")"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5266492,"sourceId":8764906,"sourceType":"datasetVersion"},{"datasetId":5267445,"sourceId":8766219,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
